{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd467823-5d83-47bb-9737-5ab732e10375",
   "metadata": {},
   "source": [
    "## Principles  for designing algorithms and intuition for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e7063-c13b-4403-85b6-2062dbd34c5f",
   "metadata": {},
   "source": [
    "### Chap1 Introduction\n",
    "Bandits provide a simple model of dilemma of decision making in uncertainty and also have practical applicatons. Mathematical formulation of bandit problems leads to a rich structure with connections to other branches of mathematics including convex analysis/optimization, brownian motion, probability theory, concentration analysis, statistics, differential geometry and information theory.\n",
    "\n",
    "#### Bandit problems\n",
    "**objective**: Finding the right balance between exploration and exploitation is at the heart of all bandit problems.\\\n",
    "**settings**: All the learner knows is that the true environment lies in some set $\\mathcal E$.(e.g. markov process with unknown transition probabilities)\\\n",
    "**detailed objective**: Most of the efforts made in this book is devoted to understanding the regret: the difference between the total expected reward using policy $\\pi$ for n rounds and the total expected reward collected by the learner over n rounds, this means regret can be written in the form **regret(n)**. We choose $\\pi$ from a set of policies $\\Pi$, competitor class, which is large enough to include the optimal policy for all environments\n",
    "\n",
    "Two extreme problem setting are 'rewards are stochastic and stationary' and 'adversarial bandit'. In adversarial bandit, we drop all the assumptions on how rewards are generated except that they are chosen without knowledge of the learner’s actions and lie in a bounded set. The trick to say something meaningful in this setting is to restrict the competitor class. The learner is not expected to find the best sequence of actions, which may be like finding a needle in a haystack. Instead, we usually choose Π to be the set of constant policies and demand that the learner is not much worse than any of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90fa2-5afb-4021-83ab-efa353b64a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
